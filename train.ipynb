{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from model import GPTLanguageModel\n",
    "from preprocess import decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_rate = 3e-4\n",
    "max_iters = 10000\n",
    "eval_interval = 500\n",
    "eval_iters = 200\n",
    "block_size = 128 # TODO: Align in config\n",
    "batch_size = 4\n",
    "\n",
    "train_data = torch.load(\"assets/train.pt\")\n",
    "valid_data = torch.load(\"assets/valid.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71425"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize model & optimizer\n",
    "model = GPTLanguageModel()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learn_rate)\n",
    "\n",
    "# number of model parameters\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "n_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split, block_size)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    \n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "\n",
    "def get_batch(split, block_size):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else valid_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.2051, val loss 4.2039\n",
      "step 500: train loss 2.7864, val loss 2.7987\n",
      "step 1000: train loss 2.6321, val loss 2.6635\n",
      "step 1500: train loss 2.5605, val loss 2.5786\n",
      "step 2000: train loss 2.5336, val loss 2.5478\n",
      "step 2500: train loss 2.5367, val loss 2.5738\n",
      "step 3000: train loss 2.5517, val loss 2.6023\n",
      "step 3500: train loss 2.5854, val loss 2.6409\n",
      "step 4000: train loss 2.5938, val loss 2.6472\n",
      "step 4500: train loss 2.6030, val loss 2.6477\n",
      "step 5000: train loss 2.6316, val loss 2.6904\n",
      "step 5500: train loss 2.6532, val loss 2.7012\n",
      "step 6000: train loss 2.6225, val loss 2.7201\n",
      "step 6500: train loss 2.6593, val loss 2.7262\n",
      "step 7000: train loss 2.6827, val loss 2.7636\n",
      "step 7500: train loss 2.7191, val loss 2.7944\n",
      "step 8000: train loss 2.7411, val loss 2.7973\n",
      "step 8500: train loss 2.7755, val loss 2.8264\n",
      "step 9000: train loss 2.7687, val loss 2.8230\n",
      "step 9500: train loss 2.7786, val loss 2.8505\n",
      "step 9999: train loss 2.8231, val loss 2.9023\n"
     ]
    }
   ],
   "source": [
    "# learning iterations\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss(model)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample batch of data\n",
    "    x_batch, y_batch = get_batch('train', block_size=32)\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(x_batch, y_batch)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CAy Clak; mow, ar it a Moke, houshead your she be Of h'd t Hyound guve ave mud BeumPeruber ITin My athe s tw -ld, UmyINord w Le be Terneor o ge Bumy he Ae have outhe heAeensute Prsense Phf Foususlo I wno ICo;\n",
      "Shthilurseeshaike wicos I es Kize marre Gous Rd ne Henomnt ss Rnfind otare y Mnulint 's ine aciny k'kpene, ondswaree bin t Isuloohaky\n",
      "Bueempamearend.\n",
      "Th Maimy-\n",
      "Y o\n",
      "O ws mane fowe s My He ch h h Bucen Lore r Ve Iff htreno Wh t it f Go os Ore iot my KIs Lousthe Is beesise Gr Eo I'shacoulisono\n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "from preprocess import get_vocab\n",
    "\n",
    "text = open(\"assets/input.txt\", \"r\").read()\n",
    "vocab = get_vocab(text)\n",
    "\n",
    "context = torch.zeros((1, 1), dtype=torch.long)\n",
    "sampled = model.generate(context, max_new_tokens=500)[0]\n",
    "print(decode(sampled, vocab))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
